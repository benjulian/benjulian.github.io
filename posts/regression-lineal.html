<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fundamentos de la Regresion Lineal</title>
    
    <link rel="stylesheet" href="../style.css"> 
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Mantenemos la combinación original que te gustó -->
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,700;1,400&family=Source+Code+Pro:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- Hoja de estilos de Prism.js (tema oscuro "Tomorrow Night") -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <main class="post-full">
        <header class="post-header">
            <h1>Fundamentos de la Regresion Lineal</h1>
            <p class="post-meta">15 of October, 2025</p>
        </header>

        <article class="post-content">

          <h2><strong>El Fundamento de la Predicción: Una Introducción a la Regresión Lineal desde los Primeros Principios</strong></h2>

          <p>En el estudio de cualquier fenómeno, nuestro punto de partida es casi siempre el mismo: la observación de los datos. Imaginemos que hemos recolectado un conjunto de mediciones, donde para cada observación disponemos de un conjunto de variables de entrada y un valor de salida correspondiente. Al visualizar estos datos, por ejemplo, en un simple gráfico de dispersión, podríamos notar una tendencia reveladora: los puntos parecen agruparse en torno a una línea recta.</p>

          <p>Esta observación visual, aunque simple, es profundamente significativa. Sugiere que podría existir una relación funcional subyacente que conecta nuestras entradas con nuestras salidas. Ante esta evidencia, nos planteamos una pregunta fundamental: ¿cuál es la estructura matemática más simple y fundamental que podría describir esta tendencia? La respuesta, por supuesto, es la ecuación de una recta.</p>

          <h3><strong>La Hipótesis de Linealidad</strong></h3>

            <p>Esto nos lleva a formular nuestra hipótesis central: <strong>asumimos que la relación entre las variables de entrada y las de salida es de naturaleza lineal</strong>. Este es el nacimiento conceptual del modelo de <strong>Regresión Lineal</strong>.</p> 
            
            <p>No pretendemos que los datos se ajusten perfectamente a una línea; el mundo real es inherentemente ruidoso y complejo. En cambio, nuestro objetivo es encontrar la línea "óptima" que mejor represente la tendencia general contenida en los datos, minimizando la distancia promedio entre la línea y cada punto de datos.</p>
  
            <p>En un caso simple con una sola variable de entrada, esta idea se materializa en la conocida ecuación \(y = mx + b\). Sin embargo, en el dominio de la inteligencia artificial, rara vez trabajamos con una única variable de entrada. Generalmente, cada muestra de nuestros datos está definida por un conjunto de \(b\) características o <em>features</em>. Por lo tanto, debemos generalizar nuestra noción de "línea" desde un plano bidimensional a un espacio de alta dimensionalidad. En este nuevo contexto, nuestra "línea" se convierte en un <strong>hiperplano</strong>.</p>
  
            <p>La formalización matemática de esta transformación lineal, que constituye el corazón de nuestro modelo, se expresa de la siguiente manera:</p>
            
            \[ Z_{ac} = X_{ab}W_{bc} + B_{ac} \]
            
            <p>Aquí, \(X\) representa nuestros datos de entrada, mientras que \(W\) (los pesos) y \(B\) (el sesgo) son los parámetros del modelo que debemos "aprender". El tensor \(Z\) contendrá las predicciones generadas por el modelo.</p>

          <h3><strong>El Objetivo: Aprender a Predecir</strong></h3>
          
            <p>El propósito del "aprendizaje" en este modelo es, precisamente, encontrar los valores numéricos para los tensores \(W\) y \(B\) que hagan que nuestras predicciones, \(Z\), sean lo más cercanas posible a los valores reales observados, que llamaremos \(Y\). Para lograr esto, nos embarcamos en un proceso iterativo conocido como el <strong>Ciclo de Entrenamiento</strong> (<em>Training Cycle</em>). Este ciclo, que exploraremos en detalle a lo largo de este artículo, se compone de cuatro etapas conceptuales:</p>
            
            <ol>
                
                <li><strong>Forward Pass (Paso hacia adelante):</strong> Utilizando una estimación inicial de \(W\) y \(B\), aplicamos la ecuación del modelo a nuestros datos de entrada \(X\) para generar un conjunto de predicciones \(Z\).</li>
                <br>
                <li><strong>Loss Computation (Cálculo de la Périda):</strong> Medimos qué tan erróneas son nuestras predicciones comparando \(Z\) con los valores verdaderos \(Y\). Esta medida de error se cuantifica en un único valor escalar llamado "périda" (<em>loss</em>).</li>
                <br>
                <li><strong>Backward Pass (Paso hacia atrás):</strong> Aquí es donde interviene el cálculo. Determinamos cómo un pequeño cambio en cada uno de los parámetros de \(W\) y \(B\) afectaría la pérdida total. Este proceso nos da los gradientes, que nos indican la dirección en la que debemos ajustar nuestros parámetros para reducir el error.</li>
                <br>
                <li><strong>Optimization (Optimización):</strong> Finalmente, actualizamos los valores de \(W\) y \(B\) moviéndolos ligeramente en la dirección opuesta a sus gradientes, acercándonos un paso más a la configuración óptima que minimiza el error.</li>
            
            </ol>
            
            <p>Este ciclo se repite una y otra vez, refinando progresivamente los parámetros del modelo hasta que nuestras predicciones sean lo más precisas posible.</p>
            
            <p>Antes de sumergirnos en la mecánica de este ciclo, es imperativo establecer un lenguaje común. Nuestro primer paso será definir formalmente las estructuras de datos, o tensores, con las que operaremos, estableciendo las convenciones de notación que nos guiarán a través de este desarrollo matemático.</p>

            <hr>

            <h2><strong>Formalización Matemática: Definiendo los Tensores del Modelo</strong></h2>
            
            <p>Para construir nuestro modelo de Regresión Lineal sobre una base rigurosa, debemos primero definir con precisión los objetos matemáticos que lo componen. En este contexto, trataremos todas nuestras estructuras de datos —entradas, salidas y parámetros— como <strong>tensores</strong>. Un tensor es una generalización de vectores y matrices a un número arbitrario de dimensiones, y nos proporciona un marco robusto para describir las operaciones de nuestro modelo.</p>
            
            <p>Para manipular estos tensores, emplearemos exclusivamente la <strong>notación de Einstein</strong>. Esta convención es a la vez elegante y poderosa: cualquier índice que aparezca repetido en un mismo término implica una suma sobre todos los valores posibles de dicho índice. Por ejemplo, un término como \(X_{ab}W_{bc}\) es una forma compacta de escribir \(\sum_{b} X_{ab}W_{bc}\). Esta notación nos permitirá expresar operaciones complejas, como la multiplicación de matrices, sin la ambigüedad de la notación matricial tradicional, enfocándonos en la interacción entre las dimensiones.</p>
            
            <h3><strong>Los Datos: Entrada y Salida</strong></h3>
            
            <p>Nuestros datos consisten en dos componentes principales: las observaciones de entrada y los valores objetivo correspondientes.</p>
            
            <ul>
                <li><strong>El Tensor de Entrada \(X\):</strong> Este tensor contiene el conjunto completo de nuestras observaciones. Lo definimos como un tensor de rango 2 (una matriz) de dimensiones \(a \times b\):
                    
                    \[ X \in \mathbb{R}^{a \times b} \]
                    
                    Los índices tienen un significado preciso:
                    
                    <br>
                    
                    <ul>
                        <li>El índice \(a\) recorre las <strong>muestras</strong> (<em>samples</em>) de nuestro conjunto de datos, desde \(1\) hasta el número total de observaciones.</li>

                        <br>
                        
                        <li>El índice \(b\) recorre las <strong>características</strong> (<em>features</em>) que describen cada muestra.</li>
                    </ul>

                    <br>
                    
                    Este tensor se veria (de manera general) asi:

                    \[
                    X = \begin{bmatrix}
                        x_{11} & x_{12} & \cdots & x_{1b} \\
                        x_{21} & x_{22} & \cdots & x_{2b} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        x_{a1} & x_{a2} & \cdots & x_{ab}
                    \end{bmatrix}
                    \]
                    
                    Así, un elemento escalar \(X_{ab}\) representa el valor de la \(b\)-ésima característica para la \(a\)-ésima muestra de nuestros datos.
                </li>
                
                <br>
                
                <li><strong>El Tensor Objetivo \(Y\):</strong> Este tensor alberga los valores verdaderos que nuestro modelo aspira a predecir. Al igual que \(X\), es un tensor de rango 2, pero con dimensiones \(a \times c\):
                    
                    \[ Y \in \mathbb{R}^{a \times c} \]
                    
                    El significado de sus índices es el siguiente:

                    <br><br>
                    
                    <ul>
                        <li>El índice \(a\) corresponde directamente al índice de muestras de \(X\), asegurando una alineación perfecta entre cada entrada y su salida esperada.</li>
                        
                        <br>

                        <li>El índice \(c\) recorre las <strong>clases</strong> o variables de salida. En el caso más simple de regresión, \(c\) podría ser 1, pero esta notación nos permite manejar modelos que predicen múltiples valores simultáneamente.</li>
                    </ul>

                    <br>
                    
                    Este tensor se veria (de manera general) asi:

                    \[
                    Y = \begin{bmatrix}
                        y_{11} & y_{12} & \cdots & y_{1c} \\
                        y_{21} & y_{22} & \cdots & y_{2c} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        y_{a1} & y_{a2} & \cdots & y_{ac}
                    \end{bmatrix}
                    \]
                    
                    Un elemento \(Y_{ac}\) es, por tanto, el valor real de la \(c\)-ésima variable de salida para la \(a\)-ésima muestra.
                </li>
            </ul>
            
            <h3><strong>Los Parámetros del Modelo: Pesos y Sesgo</strong></h3>
            
            <p>Los parámetros del modelo son los valores numéricos que no se derivan de los datos, sino que son "aprendidos" durante el proceso de entrenamiento. Son las "perillas" que ajustaremos para minimizar el error de predicción.</p>
            
            <ul>
                <li><strong>El Tensor de Pesos \(W\):</strong> El tensor de pesos \(W\) captura la fuerza y el signo de la relación entre cada característica de entrada y cada clase de salida. Se define como un tensor de rango 2 de dimensiones \(b \times c\):
                    
                    \[ W \in \mathbb{R}^{b \times c} \]
                    
                    La lógica de sus índices es crucial para la conexión entre entrada y salida:
                    
                    <br>
                    
                    <ul>
                        <li>El índice \(b\) se alinea con las \(b\) características del tensor de entrada \(X\).</li>
                        
                        <br>
                        
                        <li>El índice \(c\) se alinea con las \(c\) clases del tensor de salida \(Y\).</li>
                    </ul>

                    <br>
                    
                    Este tensor se veria (de manera general) asi:

                    \[
                    W = \begin{bmatrix}
                        w_{11} & w_{12} & \cdots & w_{1c} \\
                        w_{21} & w_{22} & \cdots & w_{2c} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        w_{b1} & w_{b2} & \cdots & w_{bc}
                    \end{bmatrix}
                    \]
                    
                    De este modo, un elemento escalar \(W_{bc}\) representa el peso que modula la influencia de la \(b\)-ésima característica de entrada sobre la \(c\)-ésima predicción de salida.
                </li>
                
                <br>
                
                <li><strong>El Tensor de Sesgo \(B\):</strong> El tensor de sesgo, también conocido como <em>bias</em> o intercepto, actúa como un término de desplazamiento. Permite que el hiperplano del modelo se ajuste verticalmente, sin necesidad de pasar por el origen del espacio de características. Lo definimos con dimensiones \(a \times c\):
                    
                    \[ B \in \mathbb{R}^{a \times c} \]

                    El tensor de sesgo se crea a partir de 2 tensores de orden 2:

                    <br>
                    
                    <ul>
                        <li>
                            Tensor "\(b\)", el cual tiene dimensiones \( 1 \times c\), donde los c-ésimos valores corresponden al sesgo que va a tener cada una de las "neuronas";
                        </li>
                        
                        <br>
                        
                        <li>
                            Tensor "\( 1 \)", el cual tiene dimensiones \( a \times 1 \) y sus valores son simplemente unos.
                        </li>
                    </ul>
                    
                    Con estos 2 "sub-tensores", podemos construir el tensor final \( B \), de manera que cada columna tenga los mismos valores (correspondiente a cada "neurona").

                    Esta operación se realiza (en notación de Einstein) de la siguiente manera:

                    \[
                    B_{ac} = 1_{a \times 1} b_{1 \times c} = \sum 1_{a \times 1} \cdot b_{1 \times c}
                    \]

                    De forma más visual, la generalización sería así:

                    \[
                    B_{ac} = 1_{a \times 1} b_{1 \times c}
                    =
                    \begin{bmatrix}
                        b_{11} & b_{12} & \cdots & b_{1c} \\
                        b_{11} & b_{12} & \cdots & b_{1c} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        b_{11} & b_{12} & \cdots & b_{1c}
                    \end{bmatrix}
                    \]

                    De esta forma, la ecuacion de la regresion lineal seria asi:

                    \[
                    Z_{ac} = X_{ab} \; W_{bc} + 1_{a \times 1} b_{1 \times c} = \left\{ \sum_{b=1}^{b} X_{ab} \times W_{bc} \right\} + 1_{a \times 1} \; b_{1 \times c}
                    \]
                    
                </li>
            </ul>
          
